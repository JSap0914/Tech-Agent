# Tech Spec Agent LangGraph ìƒì„¸ ì„¤ê³„ ë° ì‹œê°í™”

## ğŸ“‹ ëª©ì°¨
1. [ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”](#1-ì „ì²´-ì›Œí¬í”Œë¡œìš°-ì‹œê°í™”)
2. [LangGraph ë…¸ë“œ ìƒì„¸ ì„¤ê³„](#2-langgraph-ë…¸ë“œ-ìƒì„¸-ì„¤ê³„)
3. [ìƒíƒœ ê´€ë¦¬ ìŠ¤í‚¤ë§ˆ](#3-ìƒíƒœ-ê´€ë¦¬-ìŠ¤í‚¤ë§ˆ)
4. [ì¡°ê±´ë¶€ ë¶„ê¸° ë¡œì§](#4-ì¡°ê±´ë¶€-ë¶„ê¸°-ë¡œì§)
5. [êµ¬í˜„ ì½”ë“œ](#5-êµ¬í˜„-ì½”ë“œ)
6. [ë°ì´í„° íë¦„ ë‹¤ì´ì–´ê·¸ë¨](#6-ë°ì´í„°-íë¦„-ë‹¤ì´ì–´ê·¸ë¨)
7. [ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬](#7-ì—ëŸ¬-ì²˜ë¦¬-ë°-ë³µêµ¬)
8. [ì„±ëŠ¥ ìµœì í™”](#8-ì„±ëŠ¥-ìµœì í™”)

---

## 1. ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”

### 1.1 ë©”ì¸ í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    START([ì‹œì‘: ë””ìì¸ ì™„ë£Œ]) --> LOAD[1. load_inputs<br/>PRD/ë””ìì¸ ë¡œë“œ]
    
    LOAD --> ANALYZE[2. analyze_completeness<br/>ê¸°íš ì™„ì „ì„± ë¶„ì„]
    
    ANALYZE --> CHECK_SCORE{ì™„ì „ì„± ì ìˆ˜<br/> >= 80?}
    
    CHECK_SCORE -->|Yes| IDENTIFY[3. identify_tech_gaps<br/>ê¸°ìˆ  gap íƒì§€]
    CHECK_SCORE -->|No| ASK_USER[3-1. ask_user_clarification<br/>ì‚¬ìš©ì ì§ˆë¬¸]
    
    ASK_USER --> IDENTIFY
    
    IDENTIFY --> HAS_GAPS{ê¸°ìˆ  gap<br/>ì¡´ì¬?}
    
    HAS_GAPS -->|Yes| RESEARCH[4. research_technologies<br/>ì˜¤í”ˆì†ŒìŠ¤ ì›¹ ê²€ìƒ‰]
    HAS_GAPS -->|No| PARSE_CODE
    
    RESEARCH --> PRESENT[5. present_options<br/>ì„ íƒì§€ ì œì‹œ]
    
    PRESENT --> WAIT[6. wait_user_decision<br/>ì‚¬ìš©ì ì„ íƒ ëŒ€ê¸°]
    
    WAIT --> VALIDATE_DECISION[7. validate_decision<br/>ì„ íƒ ê²€ì¦]
    
    VALIDATE_DECISION --> HAS_CONFLICT{ì¶©ëŒ<br/>ìˆìŒ?}
    
    HAS_CONFLICT -->|Yes| WARN[7-1. warn_user<br/>ê²½ê³  í‘œì‹œ]
    HAS_CONFLICT -->|No| MORE_GAPS
    
    WARN --> USER_CONFIRM{ì‚¬ìš©ì<br/>í™•ì¸?}
    USER_CONFIRM -->|ì¬ì„ íƒ| PRESENT
    USER_CONFIRM -->|ê³„ì†| MORE_GAPS
    
    MORE_GAPS{ë‚¨ì€ ê²°ì •<br/>ìˆìŒ?}
    
    MORE_GAPS -->|Yes| RESEARCH
    MORE_GAPS -->|No| PARSE_CODE[8. parse_ai_studio_code<br/>Google AI Studio ì½”ë“œ ë¶„ì„]
    
    PARSE_CODE --> INFER_API[9. infer_api_spec<br/>API ëª…ì„¸ ì¶”ë¡ ]
    
    INFER_API --> GEN_TRD[10. generate_trd<br/>TRD ìƒì„±]
    
    GEN_TRD --> VALIDATE_TRD[11. validate_trd<br/>TRD ê²€ì¦]
    
    VALIDATE_TRD --> TRD_VALID{TRD<br/>ìœ íš¨?}
    
    TRD_VALID -->|No| GEN_TRD
    TRD_VALID -->|Yes| GEN_API[12. generate_api_spec<br/>API ëª…ì„¸ ìƒì„±]
    
    GEN_API --> GEN_DB[13. generate_db_schema<br/>DB ìŠ¤í‚¤ë§ˆ ìƒì„±]
    
    GEN_DB --> GEN_ARCH[14. generate_architecture<br/>ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨]
    
    GEN_ARCH --> GEN_TECH_STACK[15. generate_tech_stack_doc<br/>ê¸°ìˆ  ìŠ¤íƒ ë¬¸ì„œ]
    
    GEN_TECH_STACK --> SAVE[16. save_to_db<br/>DB ì €ì¥]
    
    SAVE --> NOTIFY[17. notify_next_agent<br/>ë°±ë¡œê·¸ Agent íŠ¸ë¦¬ê±°]
    
    NOTIFY --> END([ì™„ë£Œ: ë°±ë¡œê·¸ ë‹¨ê³„ë¡œ])
    
    style START fill:#e1f5e1
    style END fill:#e1f5e1
    style CHECK_SCORE fill:#fff3cd
    style HAS_GAPS fill:#fff3cd
    style HAS_CONFLICT fill:#fff3cd
    style MORE_GAPS fill:#fff3cd
    style TRD_VALID fill:#fff3cd
    style USER_CONFIRM fill:#fff3cd
    style RESEARCH fill:#cfe2ff
    style PARSE_CODE fill:#cfe2ff
    style GEN_TRD fill:#d1e7dd
    style GEN_API fill:#d1e7dd
    style GEN_DB fill:#d1e7dd
    style GEN_ARCH fill:#d1e7dd
    style SAVE fill:#f8d7da
```

### 1.2 4ë‹¨ê³„ Phase êµ¬ì¡°

```mermaid
graph LR
    subgraph Phase1[Phase 1: ì…ë ¥ ë° ë¶„ì„]
        N1[load_inputs]
        N2[analyze_completeness]
        N3[identify_tech_gaps]
    end
    
    subgraph Phase2[Phase 2: ê¸°ìˆ  ì¡°ì‚¬ ë° ì„ íƒ]
        N4[research_technologies]
        N5[present_options]
        N6[wait_user_decision]
        N7[validate_decision]
    end
    
    subgraph Phase3[Phase 3: ì½”ë“œ ë¶„ì„ ë° ì¶”ë¡ ]
        N8[parse_ai_studio_code]
        N9[infer_api_spec]
    end
    
    subgraph Phase4[Phase 4: ë¬¸ì„œ ìƒì„±]
        N10[generate_trd]
        N11[validate_trd]
        N12[generate_api_spec]
        N13[generate_db_schema]
        N14[generate_architecture]
        N15[generate_tech_stack_doc]
        N16[save_to_db]
    end
    
    Phase1 --> Phase2
    Phase2 --> Phase3
    Phase3 --> Phase4
    
    style Phase1 fill:#e3f2fd
    style Phase2 fill:#fff3e0
    style Phase3 fill:#f3e5f5
    style Phase4 fill:#e8f5e9
```

### 1.3 ìƒíƒœ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
stateDiagram-v2
    [*] --> Initializing: ì„¸ì…˜ ì‹œì‘
    
    Initializing --> Analyzing: PRD/ë””ìì¸ ë¡œë“œ ì™„ë£Œ
    
    Analyzing --> Clarifying: ì™„ì „ì„± ë‚®ìŒ (< 80ì )
    Analyzing --> Researching: ì™„ì „ì„± ì–‘í˜¸ (>= 80ì )
    
    Clarifying --> Analyzing: ì‚¬ìš©ì ë‹µë³€ ë°˜ì˜
    
    Researching --> Presenting: ì›¹ ê²€ìƒ‰ ì™„ë£Œ
    
    Presenting --> WaitingDecision: ì˜µì…˜ ì œì‹œ
    
    WaitingDecision --> Validating: ì‚¬ìš©ì ì„ íƒ
    
    Validating --> Warning: ì¶©ëŒ ê°ì§€
    Validating --> Researching: ì¶”ê°€ ê²°ì • í•„ìš”
    Validating --> Parsing: ëª¨ë“  ê²°ì • ì™„ë£Œ
    
    Warning --> Presenting: ì¬ì„ íƒ
    Warning --> Parsing: ê³„ì† ì§„í–‰
    
    Parsing --> Inferring: ì½”ë“œ ë¶„ì„ ì™„ë£Œ
    
    Inferring --> Generating: API ì¶”ë¡  ì™„ë£Œ
    
    Generating --> Validating_TRD: TRD ì´ˆì•ˆ ìƒì„±
    
    Validating_TRD --> Generating: ê²€ì¦ ì‹¤íŒ¨
    Validating_TRD --> Documenting: ê²€ì¦ ì„±ê³µ
    
    Documenting --> Saving: 5ì¢… ë¬¸ì„œ ìƒì„±
    
    Saving --> Completed: DB ì €ì¥ ì™„ë£Œ
    
    Completed --> [*]: ë°±ë¡œê·¸ Agent íŠ¸ë¦¬ê±°
    
    note right of Researching
        ì›¹ ê²€ìƒ‰ ìµœëŒ€ 3íšŒ
        ê° gapë§ˆë‹¤ 3ê°œ ì˜µì…˜
    end note
    
    note right of Validating
        ê¸°ìˆ  ì¶©ëŒ ì²´í¬
        ìš”êµ¬ì‚¬í•­ í˜¸í™˜ì„± ê²€ì¦
    end note
    
    note right of Generating
        ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        í’ˆì§ˆ ì ìˆ˜ >= 90 í•„ìš”
    end note
```

---

## 2. LangGraph ë…¸ë“œ ìƒì„¸ ì„¤ê³„

### 2.1 Node 1: load_inputs

**ëª©ì **: í”„ë¡œì íŠ¸ ë°ì´í„°ë¥¼ DBì—ì„œ ë¡œë“œí•˜ê³  ì´ˆê¸°í™”

```python
async def load_inputs_node(state: TechSpecState) -> TechSpecState:
    """
    ì…ë ¥ ë°ì´í„° ë¡œë“œ ë…¸ë“œ
    
    - DBì—ì„œ PRD, ë””ìì¸ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    - Google AI Studio ì½”ë“œ ê²½ë¡œ í™•ì¸
    - ì„¸ì…˜ ì´ˆê¸°í™”
    """
    project_id = state["project_id"]
    
    # 1. PRD ë¡œë“œ
    prd = await db.fetch_one("""
        SELECT content 
        FROM documents 
        WHERE project_id = $1 AND document_type = 'prd'
    """, project_id)
    
    # 2. ë””ìì¸ ë¬¸ì„œ 5ì¢… ë¡œë“œ
    design_docs = await db.fetch_all("""
        SELECT document_type, content
        FROM documents
        WHERE project_id = $1 
        AND document_type LIKE 'design_%'
    """, project_id)
    
    # 3. ì´ˆê¸° TRD (í›„-ê¸°íšì—ì„œ ìƒì„±í•œ ê²ƒ)
    initial_trd = await db.fetch_one("""
        SELECT content
        FROM documents
        WHERE project_id = $1 AND document_type = 'initial_trd'
    """, project_id)
    
    # 4. Google AI Studio ì½”ë“œ ê²½ë¡œ
    google_code_path = await db.fetch_one("""
        SELECT file_path
        FROM design_artifacts
        WHERE project_id = $1 AND artifact_type = 'google_ai_studio_code'
    """, project_id)
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state.update({
        "prd_content": prd["content"] if prd else "",
        "design_docs": {doc["document_type"]: doc["content"] for doc in design_docs},
        "initial_trd": initial_trd["content"] if initial_trd else "",
        "google_ai_studio_code_path": google_code_path["file_path"] if google_code_path else None,
        "current_stage": "loaded",
        "completion_percentage": 5.0
    })
    
    # ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€
    welcome_message = """
    ì•ˆë…•í•˜ì„¸ìš”! Tech Spec Agentì…ë‹ˆë‹¤. ğŸ¤–
    
    PRDì™€ ë””ìì¸ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.
    ì´ì œ ê¸°ìˆ  ìŠ¤í™ì„ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ê¸°ìˆ ì  ê²°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
    
    í•¨ê»˜ ìµœì ì˜ ê¸°ìˆ  ìŠ¤íƒì„ ì„ íƒí•´ë³´ì‹œì£ !
    """
    
    state["conversation_history"].append({
        "role": "agent",
        "message": welcome_message,
        "timestamp": datetime.now()
    })
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: analyze_completeness (ë¬´ì¡°ê±´)
```

**ì…ë ¥ ìƒíƒœ**:
- `project_id`: UUID

**ì¶œë ¥ ìƒíƒœ**:
- `prd_content`: str
- `design_docs`: Dict[str, str]
- `initial_trd`: str
- `google_ai_studio_code_path`: str | None
- `current_stage`: "loaded"
- `completion_percentage`: 5.0

**ë‹¤ìŒ ë…¸ë“œ**: `analyze_completeness` (ë¬´ì¡°ê±´)

---

### 2.2 Node 2: analyze_completeness

**ëª©ì **: PRDì™€ ë””ìì¸ ë¬¸ì„œì˜ ì™„ì „ì„± í‰ê°€

```python
async def analyze_completeness_node(state: TechSpecState) -> TechSpecState:
    """
    ê¸°íš ì™„ì „ì„± ë¶„ì„ ë…¸ë“œ
    
    - PRDì™€ ë””ìì¸ ë¬¸ì„œë¥¼ ì¢…í•© ë¶„ì„
    - 8ê°œ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ì‚°ì •
    - ëˆ„ë½/ëª¨í˜¸í•œ ìš”ì†Œ ì‹ë³„
    """
    
    # Claudeì—ê²Œ ë¶„ì„ ìš”ì²­
    analysis_prompt = f"""
    ë‹¤ìŒ PRDì™€ ë””ìì¸ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ê¸°ìˆ  ìŠ¤í™ ì‘ì„±ì— í•„ìš”í•œ ì™„ì „ì„±ì„ í‰ê°€í•˜ì„¸ìš”.
    
    <prd>
    {state["prd_content"]}
    </prd>
    
    <design_docs>
    {json.dumps(state["design_docs"], indent=2, ensure_ascii=False)}
    </design_docs>
    
    ë‹¤ìŒ 8ê°œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ê°ê° 0-100ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ì´ì ì„ ê³„ì‚°í•˜ì„¸ìš”:
    
    1. **ì¸ì¦ ì‹œìŠ¤í…œ** (0-15ì )
       - ì‚¬ìš©ì ì¸ì¦ ë°©ì‹ì´ ëª…ì‹œë˜ì—ˆëŠ”ê°€?
       - ì†Œì…œ ë¡œê·¸ì¸ ìš”êµ¬ì‚¬í•­ì´ ìˆëŠ”ê°€?
       - ê¶Œí•œ ê´€ë¦¬ ì •ì±…ì´ ì •ì˜ë˜ì—ˆëŠ”ê°€?
    
    2. **API êµ¬ì¡°** (0-15ì )
       - í•„ìš”í•œ API ì—”ë“œí¬ì¸íŠ¸ê°€ ì‹ë³„ë˜ì—ˆëŠ”ê°€?
       - ìš”ì²­/ì‘ë‹µ ë°ì´í„° êµ¬ì¡°ê°€ ëª…í™•í•œê°€?
    
    3. **ë°ì´í„° ëª¨ë¸** (0-15ì )
       - ì£¼ìš” ì—”í‹°í‹°ê°€ ì •ì˜ë˜ì—ˆëŠ”ê°€?
       - ì—”í‹°í‹° ê°„ ê´€ê³„ê°€ ëª…í™•í•œê°€?
    
    4. **íŒŒì¼ ì²˜ë¦¬** (0-10ì )
       - íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ìš”êµ¬ì‚¬í•­ì´ ìˆëŠ”ê°€?
       - ì €ì¥ ë°©ì‹ì´ ëª…ì‹œë˜ì—ˆëŠ”ê°€?
    
    5. **ì‹¤ì‹œê°„ ê¸°ëŠ¥** (0-10ì )
       - ì‹¤ì‹œê°„ í†µì‹ ì´ í•„ìš”í•œê°€?
       - ì–´ë–¤ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë™ê¸°í™”í•˜ëŠ”ê°€?
    
    6. **ì™¸ë¶€ ì—°ë™** (0-15ì )
       - ê²°ì œ, ì´ë©”ì¼, SMS ë“± ì™¸ë¶€ ì„œë¹„ìŠ¤ í•„ìš”í•œê°€?
       - ì—°ë™ ë°©ì‹ì´ ëª…ì‹œë˜ì—ˆëŠ”ê°€?
    
    7. **ì—ëŸ¬ ì²˜ë¦¬** (0-10ì )
       - ì£¼ìš” ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì •ì˜ë˜ì—ˆëŠ”ê°€?
    
    8. **ë³´ì•ˆ ë° ì„±ëŠ¥** (0-10ì )
       - ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ì´ ëª…ì‹œë˜ì—ˆëŠ”ê°€?
       - ì„±ëŠ¥ ëª©í‘œê°€ ì •ì˜ë˜ì—ˆëŠ”ê°€?
    
    JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
    {{
      "scores": {{
        "authentication": 0-15,
        "api_structure": 0-15,
        "data_model": 0-15,
        "file_handling": 0-10,
        "realtime": 0-10,
        "external_integration": 0-15,
        "error_handling": 0-10,
        "security_performance": 0-10
      }},
      "total_score": 0-100,
      "missing_elements": [
        "ì¸ì¦ ì‹œìŠ¤í…œ ëª…ì‹œ í•„ìš”",
        "íŒŒì¼ ì €ì¥ ë°©ì‹ ë¯¸ì •",
        ...
      ],
      "ambiguous_elements": [
        "ì‹¤ì‹œê°„ ì•Œë¦¼ ë²”ìœ„ ë¶ˆëª…í™•",
        ...
      ]
    }}
    """
    
    result = await call_claude(analysis_prompt, model="claude-sonnet-4-20250514")
    analysis = json.loads(result.content[0].text)
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state.update({
        "completeness_score": analysis["total_score"],
        "missing_elements": analysis["missing_elements"],
        "ambiguous_elements": analysis["ambiguous_elements"],
        "current_stage": "analyzed",
        "completion_percentage": 15.0
    })
    
    # ì‚¬ìš©ìì—ê²Œ í”¼ë“œë°±
    if analysis["total_score"] < 80:
        message = f"""
        ê¸°íš ë¬¸ì„œ ë¶„ì„ ê²°ê³¼, ì™„ì „ì„± ì ìˆ˜ê°€ **{analysis["total_score"]}/100**ì…ë‹ˆë‹¤.
        
        ëª‡ ê°€ì§€ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤:
        
        **ëˆ„ë½ëœ ìš”ì†Œ**:
        {format_list(analysis["missing_elements"])}
        
        **ëª¨í˜¸í•œ ìš”ì†Œ**:
        {format_list(analysis["ambiguous_elements"])}
        
        ì´ì œ ëˆ„ë½ëœ ë¶€ë¶„ì— ëŒ€í•´ ì§ˆë¬¸ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
        """
    else:
        message = f"""
        ê¸°íš ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ! ì™„ì „ì„± ì ìˆ˜: **{analysis["total_score"]}/100** âœ…
        
        ì´ì œ ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.
        """
    
    state["conversation_history"].append({
        "role": "agent",
        "message": message,
        "timestamp": datetime.now()
    })
    
    return state

# ì¡°ê±´ë¶€ ë¶„ê¸°:
# - score >= 80: identify_tech_gapsë¡œ
# - score < 80: ask_user_clarificationìœ¼ë¡œ
```

**ì…ë ¥ ìƒíƒœ**:
- `prd_content`
- `design_docs`

**ì¶œë ¥ ìƒíƒœ**:
- `completeness_score`: float (0-100)
- `missing_elements`: List[str]
- `ambiguous_elements`: List[str]

**ë‹¤ìŒ ë…¸ë“œ**: 
- `completeness_score >= 80` â†’ `identify_tech_gaps`
- `completeness_score < 80` â†’ `ask_user_clarification`

---

### 2.3 Node 2-1: ask_user_clarification (ì¡°ê±´ë¶€)

**ëª©ì **: ì™„ì „ì„±ì´ ë‚®ì„ ë•Œ ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ ìš”ì²­

```python
async def ask_user_clarification_node(state: TechSpecState) -> TechSpecState:
    """
    ì‚¬ìš©ì ëª…í™•í™” ì§ˆë¬¸ ë…¸ë“œ
    
    - ëˆ„ë½ëœ ìš”ì†Œì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì§ˆë¬¸
    - ê°ê´€ì‹ ë˜ëŠ” ì£¼ê´€ì‹ ì§ˆë¬¸ ìƒì„±
    """
    
    missing = state["missing_elements"]
    ambiguous = state["ambiguous_elements"]
    
    # ì•„ì§ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ì²« ë²ˆì§¸ í•­ëª©
    clarification_queue = state.get("clarification_queue", missing + ambiguous)
    
    if not clarification_queue:
        # ëª¨ë“  ì§ˆë¬¸ ì™„ë£Œ
        state["current_stage"] = "clarified"
        return state
    
    current_item = clarification_queue[0]
    
    # í•­ëª© ìœ í˜•ì— ë”°ë¼ ì§ˆë¬¸ ìƒì„±
    question = generate_clarification_question(current_item, state["prd_content"])
    
    state["current_question"] = question
    state["clarification_queue"] = clarification_queue[1:]  # íì—ì„œ ì œê±°
    
    state["conversation_history"].append({
        "role": "agent",
        "message": question,
        "timestamp": datetime.now(),
        "expecting_user_input": True
    })
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: wait_for_user_input (ì‹œìŠ¤í…œì´ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦¼)
# ì‚¬ìš©ì ì…ë ¥ í›„: ë‹¤ì‹œ analyze_completenessë¡œ (ì—…ë°ì´íŠ¸ëœ ì •ë³´ë¡œ ì¬ë¶„ì„)
```

---

### 2.4 Node 3: identify_tech_gaps

**ëª©ì **: ê¸°ìˆ ì ìœ¼ë¡œ ë¯¸ì •ì¸ ë¶€ë¶„ ì‹ë³„

```python
async def identify_tech_gaps_node(state: TechSpecState) -> TechSpecState:
    """
    ê¸°ìˆ  gap íƒì§€ ë…¸ë“œ
    
    - PRDì—ì„œ ê¸°ëŠ¥ì€ ëª…ì‹œë˜ì—ˆì§€ë§Œ êµ¬í˜„ ê¸°ìˆ ì´ ë¯¸ì •ì¸ ë¶€ë¶„ ì°¾ê¸°
    - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
    """
    
    gap_detection_prompt = f"""
    ë‹¤ìŒ PRDë¥¼ ë¶„ì„í•˜ì—¬, **ê¸°ëŠ¥ì€ ëª…ì‹œë˜ì–´ ìˆì§€ë§Œ êµ¬ì²´ì ì¸ êµ¬í˜„ ê¸°ìˆ ì´ ê²°ì •ë˜ì§€ ì•Šì€ ë¶€ë¶„**ì„ ì°¾ìœ¼ì„¸ìš”.
    
    <prd>
    {state["prd_content"]}
    </prd>
    
    <initial_trd>
    {state["initial_trd"]}
    </initial_trd>
    
    ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ì—ì„œ gapì„ ì°¾ìœ¼ì„¸ìš”:
    
    1. **authentication**: ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ
    2. **database**: ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
    3. **file_storage**: íŒŒì¼ ì €ì¥ì†Œ
    4. **email**: ì´ë©”ì¼ ë°œì†¡ ì„œë¹„ìŠ¤
    5. **payment**: ê²°ì œ ì‹œìŠ¤í…œ
    6. **realtime**: ì‹¤ì‹œê°„ í†µì‹ 
    7. **image_processing**: ì´ë¯¸ì§€ ì²˜ë¦¬
    8. **deployment**: ë°°í¬ í™˜ê²½
    
    ê° gapì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”:
    
    JSON í˜•ì‹:
    {{
      "gaps": [
        {{
          "id": "gap_1",
          "category": "authentication",
          "description": "ì‚¬ìš©ì ë¡œê·¸ì¸ ë° ì†Œì…œ ë¡œê·¸ì¸ êµ¬í˜„",
          "requirements": [
            "ì´ë©”ì¼/ë¹„ë°€ë²ˆí˜¸ ë¡œê·¸ì¸",
            "Google, Facebook ì†Œì…œ ë¡œê·¸ì¸",
            "JWT í† í° ê¸°ë°˜ ì¸ì¦"
          ],
          "urgency": "critical" | "high" | "medium" | "low",
          "depends_on": []  // ë‹¤ë¥¸ gap IDë“¤
        }},
        ...
      ]
    }}
    """
    
    result = await call_claude(gap_detection_prompt)
    gaps_data = json.loads(result.content[0].text)
    
    state["technical_gaps"] = gaps_data["gaps"]
    state["current_stage"] = "gaps_identified"
    state["completion_percentage"] = 25.0
    
    # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
    message = f"""
    ê¸°ìˆ  ìŠ¤íƒ ê²°ì •ì´ í•„ìš”í•œ í•­ëª©ì„ {len(gaps_data["gaps"])}ê°œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
    
    ì´ì œ ê° í•­ëª©ì— ëŒ€í•´ ìµœì ì˜ ê¸°ìˆ ì„ ì¡°ì‚¬í•˜ê³  ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
    """
    
    state["conversation_history"].append({
        "role": "agent",
        "message": message,
        "timestamp": datetime.now()
    })
    
    return state

# ì¡°ê±´ë¶€ ë¶„ê¸°:
# - len(technical_gaps) > 0: research_technologiesë¡œ
# - len(technical_gaps) == 0: parse_ai_studio_codeë¡œ (ê¸°ìˆ  ê²°ì • ë¶ˆí•„ìš”)
```

**ì¶œë ¥ ìƒíƒœ**:
- `technical_gaps`: List[Dict]

**ë‹¤ìŒ ë…¸ë“œ**:
- `len(technical_gaps) > 0` â†’ `research_technologies`
- `len(technical_gaps) == 0` â†’ `parse_ai_studio_code`

---

### 2.5 Node 4: research_technologies

**ëª©ì **: ê° ê¸°ìˆ  gapì— ëŒ€í•´ ì˜¤í”ˆì†ŒìŠ¤ ì¡°ì‚¬ ìˆ˜í–‰

```python
async def research_technologies_node(state: TechSpecState) -> TechSpecState:
    """
    ê¸°ìˆ  ì¡°ì‚¬ ë…¸ë“œ
    
    - ê° gapì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    - ìƒìœ„ 3ê°œ ì˜µì…˜ ì¶”ì¶œ
    - ì¥ë‹¨ì , ì¸ê¸°ë„, í•™ìŠµ ë‚œì´ë„ ë¶„ì„
    """
    
    gaps = state["technical_gaps"]
    pending_decisions = state.get("pending_decisions", [])
    selected_technologies = state.get("selected_technologies", {})
    
    # ì•„ì§ ê²°ì •ë˜ì§€ ì•Šì€ gapë“¤
    unresolved_gaps = [
        gap for gap in gaps 
        if gap["id"] not in selected_technologies
    ]
    
    if not unresolved_gaps:
        # ëª¨ë“  gap í•´ê²°ë¨
        state["current_stage"] = "research_complete"
        return state
    
    # ì²« ë²ˆì§¸ ë¯¸í•´ê²° gap ì²˜ë¦¬
    current_gap = unresolved_gaps[0]
    
    # 1. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ì°¾ê¸°
    search_query = f"{current_gap['description']} open source library 2025"
    search_results = await web_search(search_query)
    
    # 2. í›„ë³´ ì¶”ì¶œ (ìƒìœ„ 5ê°œ)
    candidates = await extract_candidates_from_search(
        search_results, 
        current_gap["category"],
        top_n=5
    )
    
    # 3. ê° í›„ë³´ì— ëŒ€í•œ ìƒì„¸ ì¡°ì‚¬
    detailed_options = []
    for candidate in candidates[:3]:  # ìƒìœ„ 3ê°œë§Œ
        # GitHub stats
        github_query = f"{candidate['name']} github stars"
        github_results = await web_search(github_query)
        github_stats = extract_github_stats(github_results)
        
        # ì¥ë‹¨ì  ê²€ìƒ‰
        pros_cons_query = f"{candidate['name']} advantages disadvantages comparison"
        pros_cons_results = await web_search(pros_cons_query)
        pros_cons = extract_pros_cons(pros_cons_results)
        
        # ì‚¬ìš© ì‚¬ë¡€ ê²€ìƒ‰
        use_case_query = f"{candidate['name']} real world use case example"
        use_case_results = await web_search(use_case_query)
        use_cases = extract_use_cases(use_case_results)
        
        detailed_options.append({
            "name": candidate["name"],
            "description": candidate["description"],
            "category": current_gap["category"],
            "github_stars": github_stats.get("stars", "N/A"),
            "npm_downloads": github_stats.get("npm_downloads", "N/A"),
            "last_update": github_stats.get("last_commit", "N/A"),
            "pros": pros_cons["pros"][:5],  # ìƒìœ„ 5ê°œ ì¥ì 
            "cons": pros_cons["cons"][:5],  # ìƒìœ„ 5ê°œ ë‹¨ì 
            "use_cases": use_cases[:3],
            "documentation_url": candidate.get("docs_url"),
            "learning_curve": estimate_learning_curve(candidate),
            "setup_time": estimate_setup_time(candidate),
            "cost": candidate.get("cost", "ë¬´ë£Œ")
        })
    
    # ì¡°ì‚¬ ê²°ê³¼ ì €ì¥
    research_entry = {
        "gap": current_gap,
        "options": detailed_options,
        "research_timestamp": datetime.now()
    }
    
    state["tech_research_results"].append(research_entry)
    
    # pending_decisions íì— ì¶”ê°€
    state["pending_decisions"] = pending_decisions + [current_gap["id"]]
    
    state["current_stage"] = "researched"
    state["completion_percentage"] = 30.0 + (len(state["tech_research_results"]) / len(gaps)) * 20.0
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: present_options (ë¬´ì¡°ê±´)
```

**ì¶œë ¥ ìƒíƒœ**:
- `tech_research_results`: List[Dict]
- `pending_decisions`: List[str] (gap IDs)

**ë‹¤ìŒ ë…¸ë“œ**: `present_options`

---

### 2.6 Node 5: present_options

**ëª©ì **: ì‚¬ìš©ìì—ê²Œ ê¸°ìˆ  ì„ íƒì§€ë¥¼ ëª…í™•í•˜ê²Œ ì œì‹œ

```python
async def present_options_node(state: TechSpecState) -> TechSpecState:
    """
    ì˜µì…˜ ì œì‹œ ë…¸ë“œ
    
    - ì¡°ì‚¬ëœ ê¸°ìˆ  ì˜µì…˜ì„ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í¬ë§·
    - ë¹„êµ í‘œ ìƒì„±
    - ì¶”ì²œ í‘œì‹œ
    """
    
    research_results = state["tech_research_results"]
    
    # ê°€ì¥ ìµœê·¼ ì¡°ì‚¬ ê²°ê³¼
    current_research = research_results[-1]
    gap = current_research["gap"]
    options = current_research["options"]
    
    # AI ì¶”ì²œ ê³„ì‚°
    recommendation = calculate_recommendation(
        options=options,
        project_context=state["prd_content"],
        user_level="beginner"  # ë¹„ê°œë°œì ê¸°ì¤€
    )
    
    # ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ ìƒì„±
    message = f"""
    ## {gap["description"]}ì— ëŒ€í•œ ê¸°ìˆ  ì„ íƒ
    
    ì¡°ì‚¬ ê²°ê³¼ ë‹¤ìŒ 3ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:
    
    """
    
    for i, opt in enumerate(options, 1):
        is_recommended = (opt["name"] == recommendation["name"])
        
        message += f"""
    ### {i}. {opt["name"]} {'â­ AI ì¶”ì²œ' if is_recommended else ''}
    
    **ì„¤ëª…**: {opt["description"]}
    
    âœ… **ì¥ì **:
    {format_bullet_list(opt["pros"])}
    
    âŒ **ë‹¨ì **:
    {format_bullet_list(opt["cons"])}
    
    ğŸ“Š **ë©”íŠ¸ë¦­**:
    - ì¸ê¸°ë„: {opt["github_stars"]} GitHub â­
    - ë‹¤ìš´ë¡œë“œ: {opt["npm_downloads"]}/ì›”
    - ìµœê·¼ ì—…ë°ì´íŠ¸: {opt["last_update"]}
    
    ğŸ“š **í•™ìŠµ ë‚œì´ë„**: {opt["learning_curve"]}
    â±ï¸ **ì„¤ì • ì‹œê°„**: {opt["setup_time"]}
    ğŸ’° **ë¹„ìš©**: {opt["cost"]}
    
    ğŸ“– [ë¬¸ì„œ ë³´ê¸°]({opt["documentation_url"]})
    
    ---
    """
    
    if recommendation:
        message += f"""
    
    ğŸ’¡ **AI ì¶”ì²œ ì´ìœ **:
    {recommendation["reasoning"]}
    
    """
    
    message += """
    ì–´ë–¤ ì˜µì…˜ì„ ì„ íƒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?
    - **1**, **2**, ë˜ëŠ” **3**ì„ ì…ë ¥í•˜ì„¸ìš”
    - ë‹¤ë¥¸ ê¸°ìˆ ì„ ê²€ìƒ‰í•˜ë ¤ë©´ **"ê²€ìƒ‰: <ê¸°ìˆ ëª…>"**ì„ ì…ë ¥í•˜ì„¸ìš”
    - ì˜ ëª¨ë¥´ê² ë‹¤ë©´ **"AI ì¶”ì²œ"**ì„ ì…ë ¥í•˜ì„¸ìš”
    """
    
    state["current_question"] = message
    state["current_stage"] = "presenting"
    
    state["conversation_history"].append({
        "role": "agent",
        "message": message,
        "timestamp": datetime.now(),
        "expecting_user_input": True,
        "context": {
            "gap_id": gap["id"],
            "options": [opt["name"] for opt in options]
        }
    })
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: wait_user_decision (ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°)
```

**ì¶œë ¥ ìƒíƒœ**:
- `current_question`: str (ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì§ˆ ë©”ì‹œì§€)

**ë‹¤ìŒ ë…¸ë“œ**: `wait_user_decision`

---

### 2.7 Node 6: wait_user_decision

**ëª©ì **: ì‚¬ìš©ì ì„ íƒì„ ê¸°ë‹¤ë¦¬ê³  ì…ë ¥ ì²˜ë¦¬

```python
async def wait_user_decision_node(state: TechSpecState) -> TechSpecState:
    """
    ì‚¬ìš©ì ê²°ì • ëŒ€ê¸° ë…¸ë“œ
    
    - WebSocketìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
    - ì…ë ¥ ìœ íš¨ì„± ê²€ì¦
    - ì„ íƒ ê¸°ë¡
    """
    
    # ì´ ë…¸ë“œëŠ” ì‹¤ì œë¡œëŠ” WebSocket handlerì—ì„œ ì²˜ë¦¬ë¨
    # LangGraphì—ì„œëŠ” interruptë¡œ êµ¬í˜„
    
    # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° (ì‹œìŠ¤í…œì´ ì¼ì‹œ ì¤‘ì§€)
    user_input = await wait_for_websocket_message(state["session_id"])
    
    # ì…ë ¥ íŒŒì‹±
    current_research = state["tech_research_results"][-1]
    options = current_research["options"]
    gap_id = current_research["gap"]["id"]
    
    selected_tech = None
    
    if user_input.lower() == "ai ì¶”ì²œ":
        # AIê°€ ì¶”ì²œí•œ ì˜µì…˜ ì„ íƒ
        recommendation = calculate_recommendation(options, state["prd_content"])
        selected_tech = recommendation["name"]
        selection_reason = f"AI ì¶”ì²œ: {recommendation['reasoning']}"
    
    elif user_input.startswith("ê²€ìƒ‰:"):
        # ì‚¬ìš©ìê°€ ì§ì ‘ ê¸°ìˆ ëª… ì…ë ¥
        tech_name = user_input.replace("ê²€ìƒ‰:", "").strip()
        # ì¬ê²€ìƒ‰ ë¡œì§ (research_technologiesë¡œ ë‹¤ì‹œ ì´ë™)
        state["custom_search_query"] = tech_name
        state["current_stage"] = "custom_search"
        return state
    
    elif user_input in ["1", "2", "3"]:
        # ì˜µì…˜ ì„ íƒ
        idx = int(user_input) - 1
        selected_tech = options[idx]["name"]
        selection_reason = "ì‚¬ìš©ì ì§ì ‘ ì„ íƒ"
    
    else:
        # ì˜ëª»ëœ ì…ë ¥
        state["conversation_history"].append({
            "role": "agent",
            "message": "ì˜¬ë°”ë¥¸ ì˜µì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš” (1, 2, 3 ë˜ëŠ” 'AI ì¶”ì²œ')",
            "timestamp": datetime.now()
        })
        # ë‹¤ì‹œ present_optionsë¡œ
        return state
    
    # ì„ íƒ ê¸°ë¡
    state["selected_technologies"][gap_id] = {
        "name": selected_tech,
        "category": current_research["gap"]["category"],
        "reason": selection_reason,
        "timestamp": datetime.now()
    }
    
    state["conversation_history"].append({
        "role": "user",
        "message": user_input,
        "timestamp": datetime.now()
    })
    
    state["conversation_history"].append({
        "role": "agent",
        "message": f"âœ… **{selected_tech}**ë¥¼ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.",
        "timestamp": datetime.now()
    })
    
    state["current_stage"] = "decision_made"
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: validate_decision
```

**ì¶œë ¥ ìƒíƒœ**:
- `selected_technologies`: Dict[str, Dict]

**ë‹¤ìŒ ë…¸ë“œ**: `validate_decision`

---

### 2.8 Node 7: validate_decision

**ëª©ì **: ì„ íƒí•œ ê¸°ìˆ ì´ ìš”êµ¬ì‚¬í•­ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ”ì§€ ê²€ì¦

```python
async def validate_decision_node(state: TechSpecState) -> TechSpecState:
    """
    ê¸°ìˆ  ì„ íƒ ê²€ì¦ ë…¸ë“œ
    
    - ì„ íƒí•œ ê¸°ìˆ ì´ PRD ìš”êµ¬ì‚¬í•­ê³¼ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸
    - ë‹¤ë¥¸ ì„ íƒí•œ ê¸°ìˆ ë“¤ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
    """
    
    # ê°€ì¥ ìµœê·¼ ì„ íƒ
    current_research = state["tech_research_results"][-1]
    gap_id = current_research["gap"]["id"]
    selected = state["selected_technologies"][gap_id]
    
    # ê²€ì¦ í”„ë¡¬í”„íŠ¸
    validation_prompt = f"""
    ì‚¬ìš©ìê°€ ë‹¤ìŒ ê¸°ìˆ ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤:
    
    ê¸°ìˆ : {selected["name"]}
    ì¹´í…Œê³ ë¦¬: {selected["category"]}
    
    ì´ ì„ íƒì´ ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”:
    
    <prd>
    {state["prd_content"]}
    </prd>
    
    <already_selected>
    {json.dumps(state["selected_technologies"], indent=2, ensure_ascii=False)}
    </already_selected>
    
    ë‹¤ìŒ ì‚¬í•­ì„ ì²´í¬í•˜ì„¸ìš”:
    1. PRD ìš”êµ¬ì‚¬í•­ê³¼ì˜ í˜¸í™˜ì„±
    2. ì´ë¯¸ ì„ íƒí•œ ë‹¤ë¥¸ ê¸°ìˆ ê³¼ì˜ í˜¸í™˜ì„±
    3. ê¸°ìˆ ì  ì œì•½ì‚¬í•­
    
    JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
    {{
      "is_valid": true | false,
      "conflicts": [
        {{
          "type": "requirement_mismatch" | "tech_incompatibility",
          "description": "...",
          "severity": "critical" | "warning"
        }}
      ],
      "recommendation": "ê³„ì† ì§„í–‰" | "ì¬ì„ íƒ ê¶Œì¥" | "ì¬ì„ íƒ í•„ìˆ˜"
    }}
    """
    
    result = await call_claude(validation_prompt)
    validation = json.loads(result.content[0].text)
    
    if validation["conflicts"]:
        # ì¶©ëŒ ë°œê²¬
        state["validation_warnings"] = validation["conflicts"]
        state["current_stage"] = "validation_conflict"
        
        # ì‚¬ìš©ìì—ê²Œ ê²½ê³ 
        warning_message = f"""
        âš ï¸ **ê²½ê³ **: {selected["name"]} ì„ íƒì— ì ì¬ì  ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.
        
        """
        
        for conflict in validation["conflicts"]:
            severity_emoji = "ğŸ”´" if conflict["severity"] == "critical" else "ğŸŸ¡"
            warning_message += f"""
        {severity_emoji} **{conflict["type"]}**
        {conflict["description"]}
        
        """
        
        warning_message += f"""
        
        **AI ê¶Œì¥**: {validation["recommendation"]}
        
        ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
        - **ê³„ì†**: ì´ëŒ€ë¡œ ì§„í–‰
        - **ì¬ì„ íƒ**: ë‹¤ë¥¸ ê¸°ìˆ  ì„ íƒ
        """
        
        state["current_question"] = warning_message
        state["conversation_history"].append({
            "role": "agent",
            "message": warning_message,
            "timestamp": datetime.now(),
            "expecting_user_input": True
        })
        
    else:
        # ê²€ì¦ í†µê³¼
        state["current_stage"] = "validated"
        state["conversation_history"].append({
            "role": "agent",
            "message": f"âœ… {selected['name']} ì„ íƒì´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤!",
            "timestamp": datetime.now()
        })
    
    return state

# ì¡°ê±´ë¶€ ë¶„ê¸°:
# - has_conflicts && user_wants_reselect: present_optionsë¡œ
# - has_conflicts && user_continues: check_more_gapsë¡œ
# - no_conflicts: check_more_gapsë¡œ
```

**ì¶œë ¥ ìƒíƒœ**:
- `validation_warnings`: List[Dict] (ì¶©ëŒì´ ìˆëŠ” ê²½ìš°)

**ë‹¤ìŒ ë…¸ë“œ**:
- ì¶©ëŒ ìˆìŒ + ì‚¬ìš©ìê°€ ì¬ì„ íƒ â†’ `present_options`
- ì¶©ëŒ ì—†ìŒ ë˜ëŠ” ê³„ì† ì§„í–‰ â†’ ë‚¨ì€ ê²°ì • í™•ì¸

---

### 2.9 ì¡°ê±´ ì²´í¬: check_pending_decisions

```python
def check_pending_decisions(state: TechSpecState) -> str:
    """
    ì•„ì§ ê²°ì •ë˜ì§€ ì•Šì€ ê¸°ìˆ  gapì´ ìˆëŠ”ì§€ í™•ì¸
    """
    all_gaps = state["technical_gaps"]
    selected = state["selected_technologies"]
    
    unresolved = [gap for gap in all_gaps if gap["id"] not in selected]
    
    if unresolved:
        return "more_research_needed"
    else:
        return "all_decided"

# - more_research_needed: research_technologiesë¡œ (ë‹¤ìŒ gap ì²˜ë¦¬)
# - all_decided: parse_ai_studio_codeë¡œ
```

---

### 2.10 Node 8: parse_ai_studio_code

**ëª©ì **: Google AI Studio ìƒì„± ì½”ë“œ ë¶„ì„

```python
async def parse_ai_studio_code_node(state: TechSpecState) -> TechSpecState:
    """
    Google AI Studio ì½”ë“œ íŒŒì‹± ë…¸ë“œ
    
    - ZIP íŒŒì¼ ì••ì¶• í•´ì œ
    - React ì»´í¬ë„ŒíŠ¸ AST íŒŒì‹±
    - Props, State, API í˜¸ì¶œ ì¶”ì¶œ
    """
    
    code_path = state.get("google_ai_studio_code_path")
    
    if not code_path:
        # ì½”ë“œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        state["google_ai_studio_data"] = None
        state["current_stage"] = "no_code"
        return state
    
    # 1. ZIP ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
    local_path = await download_from_s3(code_path)
    extracted_dir = extract_zip(local_path)
    
    # 2. ëª¨ë“  .tsx, .jsx íŒŒì¼ ì°¾ê¸°
    component_files = find_files(extracted_dir, patterns=["*.tsx", "*.jsx"])
    
    # 3. ê° ì»´í¬ë„ŒíŠ¸ íŒŒì‹±
    parsed_components = []
    
    for file_path in component_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            code = f.read()
        
        # TypeScript AST íŒŒì‹±
        component_data = {
            "file_path": file_path,
            "name": extract_component_name(code),
            "props": extract_props_interface(code),
            "state_vars": extract_state_variables(code),
            "api_calls": extract_api_calls(code),
            "event_handlers": extract_event_handlers(code),
            "imported_components": extract_imports(code)
        }
        
        parsed_components.append(component_data)
    
    state["google_ai_studio_data"] = {
        "components": parsed_components,
        "total_components": len(parsed_components),
        "extracted_at": datetime.now()
    }
    
    state["current_stage"] = "code_parsed"
    state["completion_percentage"] = 55.0
    
    state["conversation_history"].append({
        "role": "agent",
        "message": f"Google AI Studio ì½”ë“œ ë¶„ì„ ì™„ë£Œ: {len(parsed_components)}ê°œ ì»´í¬ë„ŒíŠ¸ ë°œê²¬",
        "timestamp": datetime.now()
    })
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: infer_api_spec
```

**ì¶œë ¥ ìƒíƒœ**:
- `google_ai_studio_data`: Dict

**ë‹¤ìŒ ë…¸ë“œ**: `infer_api_spec`

---

### 2.11 Node 9: infer_api_spec

**ëª©ì **: ì»´í¬ë„ŒíŠ¸ ë¶„ì„ìœ¼ë¡œë¶€í„° API ëª…ì„¸ ì¶”ë¡ 

```python
async def infer_api_spec_node(state: TechSpecState) -> TechSpecState:
    """
    API ëª…ì„¸ ì¶”ë¡  ë…¸ë“œ
    
    - ì»´í¬ë„ŒíŠ¸ì˜ API í˜¸ì¶œ ì½”ë“œ ë¶„ì„
    - Props êµ¬ì¡°ë¡œë¶€í„° ì‘ë‹µ ë°ì´í„° íƒ€ì… ì¶”ë¡ 
    - ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡ ìƒì„±
    """
    
    ai_studio_data = state.get("google_ai_studio_data")
    
    if not ai_studio_data:
        # ì½”ë“œê°€ ì—†ìœ¼ë©´ PRD/ë””ìì¸ë§Œìœ¼ë¡œ ì¶”ë¡ 
        inferred_api_spec = await infer_api_from_prd_only(
            state["prd_content"],
            state["design_docs"]
        )
    else:
        components = ai_studio_data["components"]
        
        inferred_endpoints = []
        
        for comp in components:
            for api_call in comp["api_calls"]:
                endpoint = {
                    "path": api_call["url"],
                    "method": api_call["method"],
                    "source_component": comp["name"],
                    "request_body_type": infer_request_type(api_call, comp),
                    "response_type": comp["props"],  # PropsëŠ” ë³´í†µ ì„œë²„ ì‘ë‹µ êµ¬ì¡°
                    "description": f"{comp['name']} ì»´í¬ë„ŒíŠ¸ì—ì„œ ì‚¬ìš©"
                }
                
                inferred_endpoints.append(endpoint)
        
        # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        inferred_api_spec = merge_duplicate_endpoints(inferred_endpoints)
    
    state["inferred_api_spec"] = inferred_api_spec
    state["current_stage"] = "api_inferred"
    state["completion_percentage"] = 65.0
    
    state["conversation_history"].append({
        "role": "agent",
        "message": f"API ëª…ì„¸ ì¶”ë¡  ì™„ë£Œ: {len(inferred_api_spec['endpoints'])}ê°œ ì—”ë“œí¬ì¸íŠ¸",
        "timestamp": datetime.now()
    })
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: generate_trd
```

**ì¶œë ¥ ìƒíƒœ**:
- `inferred_api_spec`: Dict

**ë‹¤ìŒ ë…¸ë“œ**: `generate_trd`

---

### 2.12 Node 10: generate_trd

**ëª©ì **: ìµœì¢… TRD ë¬¸ì„œ ìƒì„±

```python
async def generate_trd_node(state: TechSpecState) -> TechSpecState:
    """
    TRD ìƒì„± ë…¸ë“œ
    
    - ëª¨ë“  ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©
    - Markdown í˜•ì‹ TRD ì‘ì„±
    - ì„¹ì…˜ë³„ êµ¬ì¡°í™”
    """
    
    trd_generation_prompt = f"""
    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ TRD(Technical Requirements Document)ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    
    <prd>
    {state["prd_content"]}
    </prd>
    
    <selected_technologies>
    {json.dumps(state["selected_technologies"], indent=2, ensure_ascii=False)}
    </selected_technologies>
    
    <inferred_api_spec>
    {json.dumps(state["inferred_api_spec"], indent=2, ensure_ascii=False)}
    </inferred_api_spec>
    
    <design_docs>
    {json.dumps(state["design_docs"], indent=2, ensure_ascii=False)}
    </design_docs>
    
    ë‹¤ìŒ êµ¬ì¡°ë¡œ TRDë¥¼ ì‘ì„±í•˜ì„¸ìš”:
    
    # Technical Requirements Document (TRD)
    
    ## 1. í”„ë¡œì íŠ¸ ê°œìš”
    - í”„ë¡œì íŠ¸ëª…
    - ë²„ì „
    - ì‘ì„±ì¼
    
    ## 2. ê¸°ìˆ  ìŠ¤íƒ
    ### 2.1 í”„ë¡ íŠ¸ì—”ë“œ
    - í”„ë ˆì„ì›Œí¬: [ì„ íƒëœ ê¸°ìˆ ]
    - UI ë¼ì´ë¸ŒëŸ¬ë¦¬: [ì„ íƒëœ ê¸°ìˆ ]
    - ìƒíƒœ ê´€ë¦¬: [ì„ íƒëœ ê¸°ìˆ ]
    - ìŠ¤íƒ€ì¼ë§: [ì„ íƒëœ ê¸°ìˆ ]
    
    ### 2.2 ë°±ì—”ë“œ
    - ì–¸ì–´/í”„ë ˆì„ì›Œí¬: [ì„ íƒëœ ê¸°ìˆ ]
    - ë°ì´í„°ë² ì´ìŠ¤: [ì„ íƒëœ ê¸°ìˆ ]
    - ORM: [ì„ íƒëœ ê¸°ìˆ ]
    - ì¸ì¦: [ì„ íƒëœ ê¸°ìˆ ]
    
    ### 2.3 ì¸í”„ë¼
    - í˜¸ìŠ¤íŒ…: [ì„ íƒëœ ê¸°ìˆ ]
    - CI/CD: [ì„ íƒëœ ê¸°ìˆ ]
    
    ### 2.4 ì™¸ë¶€ ì„œë¹„ìŠ¤
    - íŒŒì¼ ìŠ¤í† ë¦¬ì§€: [ì„ íƒëœ ê¸°ìˆ ]
    - ì´ë©”ì¼: [ì„ íƒëœ ê¸°ìˆ ]
    - [ê¸°íƒ€ ì„ íƒëœ ì„œë¹„ìŠ¤ë“¤]
    
    ## 3. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
    [Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ]
    
    ## 4. API ëª…ì„¸ ê°œìš”
    [ì¶”ë¡ ëœ ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡ ìš”ì•½]
    
    ## 5. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ê°œìš”
    [ì£¼ìš” í…Œì´ë¸” ëª©ë¡]
    
    ## 6. ë³´ì•ˆ ìš”êµ¬ì‚¬í•­
    [PRD ê¸°ë°˜]
    
    ## 7. ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
    [PRD ê¸°ë°˜]
    
    ## 8. ë°°í¬ ë° ìš´ì˜
    [ì„ íƒëœ ì¸í”„ë¼ ê¸°ë°˜]
    
    Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    """
    
    result = await call_claude(
        trd_generation_prompt,
        model="claude-sonnet-4-20250514",
        max_tokens=8000
    )
    
    generated_trd = result.content[0].text
    
    state["final_trd"] = generated_trd
    state["current_stage"] = "trd_generated"
    state["completion_percentage"] = 70.0
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: validate_trd
```

**ì¶œë ¥ ìƒíƒœ**:
- `final_trd`: str (Markdown)

**ë‹¤ìŒ ë…¸ë“œ**: `validate_trd`

---

### 2.13 Node 11: validate_trd

**ëª©ì **: TRD í’ˆì§ˆ ê²€ì¦

```python
async def validate_trd_node(state: TechSpecState) -> TechSpecState:
    """
    TRD ê²€ì¦ ë…¸ë“œ
    
    - ì™„ì „ì„± ì²´í¬
    - ì¼ê´€ì„± ì²´í¬
    - í’ˆì§ˆ ì ìˆ˜ ì‚°ì •
    """
    
    validation_prompt = f"""
    ë‹¤ìŒ TRDë¥¼ ê²€ì¦í•˜ì„¸ìš”:
    
    <trd>
    {state["final_trd"]}
    </trd>
    
    ë‹¤ìŒ í•­ëª©ë“¤ì„ ì²´í¬í•˜ì„¸ìš”:
    
    1. **ì™„ì „ì„±** (0-30ì )
       - ëª¨ë“  í•„ìˆ˜ ì„¹ì…˜ì´ ìˆëŠ”ê°€?
       - ê° ì„¹ì…˜ì´ ì¶©ë¶„íˆ ìƒì„¸í•œê°€?
    
    2. **ì¼ê´€ì„±** (0-30ì )
       - PRDì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
       - ì„ íƒí•œ ê¸°ìˆ  ìŠ¤íƒì´ ëª¨ë‘ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?
    
    3. **ëª…í™•ì„±** (0-20ì )
       - ìš©ì–´ê°€ ëª…í™•í•œê°€?
       - ëª¨í˜¸í•œ í‘œí˜„ì´ ì—†ëŠ”ê°€?
    
    4. **ì‹¤í–‰ ê°€ëŠ¥ì„±** (0-20ì )
       - ê°œë°œìê°€ ì´ TRDë§Œìœ¼ë¡œ ê°œë°œ ì‹œì‘ ê°€ëŠ¥í•œê°€?
    
    JSON í˜•ì‹:
    {{
      "total_score": 0-100,
      "is_valid": true | false,  // 90ì  ì´ìƒì´ë©´ true
      "missing_sections": [...],
      "inconsistencies": [...],
      "improvement_suggestions": [...]
    }}
    """
    
    result = await call_claude(validation_prompt)
    validation = json.loads(result.content[0].text)
    
    state["trd_validation_result"] = validation
    
    if validation["is_valid"]:
        state["current_stage"] = "trd_validated"
        state["conversation_history"].append({
            "role": "agent",
            "message": f"âœ… TRD ê²€ì¦ ì™„ë£Œ! í’ˆì§ˆ ì ìˆ˜: {validation['total_score']}/100",
            "timestamp": datetime.now()
        })
    else:
        state["current_stage"] = "trd_invalid"
        state["conversation_history"].append({
            "role": "agent",
            "message": f"âŒ TRD í’ˆì§ˆì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({validation['total_score']}/100). ì¬ìƒì„±í•©ë‹ˆë‹¤...",
            "timestamp": datetime.now()
        })
    
    return state

# ì¡°ê±´ë¶€ ë¶„ê¸°:
# - is_valid == true: generate_api_specë¡œ
# - is_valid == false: generate_trdë¡œ (ì¬ìƒì„±)
```

---

### 2.14 Nodes 12-15: ë¬¸ì„œ ìƒì„± ë…¸ë“œë“¤

```python
async def generate_api_spec_node(state: TechSpecState) -> TechSpecState:
    """API ëª…ì„¸ì„œ ìƒì„± (OpenAPI/Swagger í˜•ì‹)"""
    # ... (ìƒëµ)
    state["api_specification"] = api_spec
    state["completion_percentage"] = 80.0
    return state

async def generate_db_schema_node(state: TechSpecState) -> TechSpecState:
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„± (SQL DDL)"""
    # ... (ìƒëµ)
    state["database_schema"] = db_schema
    state["completion_percentage"] = 85.0
    return state

async def generate_architecture_node(state: TechSpecState) -> TechSpecState:
    """ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (Mermaid)"""
    # ... (ìƒëµ)
    state["architecture_diagram"] = arch_diagram
    state["completion_percentage"] = 90.0
    return state

async def generate_tech_stack_doc_node(state: TechSpecState) -> TechSpecState:
    """ê¸°ìˆ  ìŠ¤íƒ ë¬¸ì„œ ìƒì„±"""
    # ... (ìƒëµ)
    state["tech_stack_document"] = tech_doc
    state["completion_percentage"] = 95.0
    return state
```

---

### 2.15 Node 16: save_to_db

**ëª©ì **: ëª¨ë“  ìƒì„±ëœ ë¬¸ì„œë¥¼ DBì— ì €ì¥

```python
async def save_to_db_node(state: TechSpecState) -> TechSpecState:
    """
    DB ì €ì¥ ë…¸ë“œ
    
    - 5ì¢… ë¬¸ì„œë¥¼ generated_trd_documents í…Œì´ë¸”ì— ì €ì¥
    - ANYON ë©”ì¸ documents í…Œì´ë¸”ì—ë„ ë³µì‚¬
    - ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
    """
    
    session_id = state["session_id"]
    project_id = state["project_id"]
    
    documents_to_save = [
        ("final_trd", state["final_trd"], "markdown"),
        ("api_specification", state["api_specification"], "yaml"),
        ("database_schema", state["database_schema"], "sql"),
        ("architecture_diagram", state["architecture_diagram"], "mermaid"),
        ("tech_stack_document", state["tech_stack_document"], "markdown")
    ]
    
    async with db.transaction():
        for doc_type, content, format in documents_to_save:
            # 1. generated_trd_documentsì— ì €ì¥
            await db.execute("""
                INSERT INTO generated_trd_documents
                (session_id, document_type, content, content_format, is_latest, validated, validation_score)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
            """, session_id, doc_type, content, format, True, True, state["trd_validation_result"]["total_score"])
            
            # 2. ANYON ë©”ì¸ documents í…Œì´ë¸”ì— ë³µì‚¬
            await db.execute("""
                INSERT INTO documents
                (project_id, document_type, content, created_by)
                VALUES ($1, $2, $3, $4)
            """, project_id, doc_type, content, "tech_spec_agent")
        
        # 3. ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
        await db.execute("""
            UPDATE tech_spec_sessions
            SET current_stage = 'completed',
                completion_percentage = 100,
                completed_at = NOW()
            WHERE id = $1
        """, session_id)
        
        # 4. í”„ë¡œì íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸
        await db.execute("""
            UPDATE projects
            SET kanban_stage = 'backlog'
            WHERE id = $1
        """, project_id)
    
    state["current_stage"] = "saved"
    state["completion_percentage"] = 100.0
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: notify_next_agent
```

---

### 2.16 Node 17: notify_next_agent

**ëª©ì **: ë°±ë¡œê·¸ Agent íŠ¸ë¦¬ê±°

```python
async def notify_next_agent_node(state: TechSpecState) -> TechSpecState:
    """
    ë‹¤ìŒ Agent ì•Œë¦¼ ë…¸ë“œ
    
    - ë°±ë¡œê·¸ Agentì—ê²Œ ì‹œì‘ ì‹ í˜¸ ì „ì†¡
    - ì‚¬ìš©ìì—ê²Œ ì™„ë£Œ ë©”ì‹œì§€
    """
    
    project_id = state["project_id"]
    
    # ë°±ë¡œê·¸ Agent íŠ¸ë¦¬ê±° (ì´ë²¤íŠ¸ ë°œí–‰)
    await event_bus.publish("tech_spec_completed", {
        "project_id": project_id,
        "session_id": state["session_id"],
        "trd_document_id": state.get("trd_document_id")
    })
    
    # ì‚¬ìš©ìì—ê²Œ ì™„ë£Œ ë©”ì‹œì§€
    completion_message = f"""
    ğŸ‰ Tech Spec ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
    
    ìƒì„±ëœ ë¬¸ì„œ:
    - âœ… TRD (Technical Requirements Document)
    - âœ… API ëª…ì„¸ì„œ
    - âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
    - âœ… ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
    - âœ… ê¸°ìˆ  ìŠ¤íƒ ë¬¸ì„œ
    
    ì´ì œ ë°±ë¡œê·¸ Agentê°€ ìë™ìœ¼ë¡œ ì‹œì‘ë©ë‹ˆë‹¤.
    Epicê³¼ Storyë¡œ ì‘ì—…ì„ ë‚˜ëˆ„ê³  ê°œë°œ ê³„íšì„ ì„¸ìš°ê² ìŠµë‹ˆë‹¤!
    """
    
    state["conversation_history"].append({
        "role": "agent",
        "message": completion_message,
        "timestamp": datetime.now()
    })
    
    state["current_stage"] = "completed"
    
    return state

# ë‹¤ìŒ ë…¸ë“œ: END
```

---

## 3. ìƒíƒœ ê´€ë¦¬ ìŠ¤í‚¤ë§ˆ

### 3.1 Complete State Schema

```python
from typing import TypedDict, List, Dict, Annotated, Optional
from datetime import datetime
import operator

class TechSpecState(TypedDict):
    """Tech Spec Agentì˜ ì „ì²´ ìƒíƒœ"""
    
    # ============ ì„¸ì…˜ ì •ë³´ ============
    session_id: str  # UUID
    project_id: str  # UUID
    user_id: str  # UUID
    
    # ============ ì…ë ¥ ë°ì´í„° ============
    prd_content: str  # PRD ì „ë¬¸
    design_docs: Dict[str, str]  # {document_type: content}
    initial_trd: str  # í›„-ê¸°íšì—ì„œ ìƒì„±í•œ ì´ˆê¸° TRD
    google_ai_studio_code_path: Optional[str]  # S3 ê²½ë¡œ
    
    # ============ ë¶„ì„ ê²°ê³¼ ============
    completeness_score: float  # 0-100
    missing_elements: List[str]  # ëˆ„ë½ëœ ìš”ì†Œë“¤
    ambiguous_elements: List[str]  # ëª¨í˜¸í•œ ìš”ì†Œë“¤
    clarification_queue: List[str]  # ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ í•­ëª©ë“¤
    
    # ============ ê¸°ìˆ  Gap ë° ì¡°ì‚¬ ============
    technical_gaps: List[Dict]  # ê¸°ìˆ ì ìœ¼ë¡œ ë¯¸ì •ì¸ ë¶€ë¶„ë“¤
    # [{
    #   "id": "gap_1",
    #   "category": "authentication",
    #   "description": "...",
    #   "requirements": [...],
    #   "urgency": "critical",
    #   "depends_on": []
    # }]
    
    tech_research_results: Annotated[List[Dict], operator.add]
    # [{
    #   "gap": {...},
    #   "options": [
    #     {
    #       "name": "NextAuth.js",
    #       "description": "...",
    #       "pros": [...],
    #       "cons": [...],
    #       "github_stars": 1000,
    #       ...
    #     }
    #   ],
    #   "research_timestamp": datetime
    # }]
    
    selected_technologies: Dict[str, Dict]
    # {
    #   "gap_1": {
    #     "name": "NextAuth.js",
    #     "category": "authentication",
    #     "reason": "ì‚¬ìš©ì ì„ íƒ",
    #     "timestamp": datetime
    #   }
    # }
    
    pending_decisions: List[str]  # ì•„ì§ ê²°ì •ë˜ì§€ ì•Šì€ gap IDë“¤
    validation_warnings: List[Dict]  # ì„ íƒ ê²€ì¦ ì‹œ ë°œê²¬ëœ ê²½ê³ ë“¤
    
    # ============ ëŒ€í™” ìƒíƒœ ============
    current_question: Optional[str]  # í˜„ì¬ ì‚¬ìš©ìì—ê²Œ ë¬»ëŠ” ì§ˆë¬¸
    conversation_history: Annotated[List[Dict], operator.add]
    # [{
    #   "role": "agent" | "user",
    #   "message": "...",
    #   "timestamp": datetime,
    #   "expecting_user_input": bool,
    #   "context": {...}
    # }]
    
    # ============ Google AI Studio ì½”ë“œ ë¶„ì„ ============
    google_ai_studio_data: Optional[Dict]
    # {
    #   "components": [
    #     {
    #       "file_path": "...",
    #       "name": "UserProfileCard",
    #       "props": {...},
    #       "state_vars": [...],
    #       "api_calls": [...],
    #       ...
    #     }
    #   ],
    #   "total_components": 10,
    #   "extracted_at": datetime
    # }
    
    inferred_api_spec: Optional[Dict]
    # {
    #   "endpoints": [
    #     {
    #       "path": "/api/users/:id",
    #       "method": "GET",
    #       "source_component": "UserProfileCard",
    #       "request_body_type": {...},
    #       "response_type": {...},
    #       "description": "..."
    #     }
    #   ]
    # }
    
    # ============ ìƒì„± ë¬¸ì„œ ============
    final_trd: Optional[str]  # Markdown
    trd_validation_result: Optional[Dict]
    # {
    #   "total_score": 95,
    #   "is_valid": true,
    #   "missing_sections": [],
    #   "inconsistencies": [],
    #   "improvement_suggestions": []
    # }
    
    api_specification: Optional[str]  # YAML (OpenAPI)
    database_schema: Optional[str]  # SQL DDL
    architecture_diagram: Optional[str]  # Mermaid
    tech_stack_document: Optional[str]  # Markdown
    
    # ============ ë©”íƒ€ ì •ë³´ ============
    current_stage: str
    # "initializing" | "loaded" | "analyzing" | "clarifying" |
    # "gaps_identified" | "researching" | "presenting" | "decision_made" |
    # "validated" | "code_parsed" | "api_inferred" | "trd_generated" |
    # "trd_validated" | "documenting" | "saved" | "completed"
    
    iteration_count: int  # TRD ì¬ìƒì„± íšŸìˆ˜
    completion_percentage: float  # 0-100
    
    started_at: datetime
    completed_at: Optional[datetime]
    
    # ============ ì—ëŸ¬ ì²˜ë¦¬ ============
    errors: Annotated[List[Dict], operator.add]
    # [{
    #   "node": "research_technologies",
    #   "error_type": "web_search_failed",
    #   "message": "...",
    #   "timestamp": datetime
    # }]
```

### 3.2 ìƒíƒœ ì´ˆê¸°í™”

```python
def create_initial_state(project_id: str, user_id: str) -> TechSpecState:
    """ì´ˆê¸° ìƒíƒœ ìƒì„±"""
    return TechSpecState(
        session_id=str(uuid.uuid4()),
        project_id=project_id,
        user_id=user_id,
        
        prd_content="",
        design_docs={},
        initial_trd="",
        google_ai_studio_code_path=None,
        
        completeness_score=0.0,
        missing_elements=[],
        ambiguous_elements=[],
        clarification_queue=[],
        
        technical_gaps=[],
        tech_research_results=[],
        selected_technologies={},
        pending_decisions=[],
        validation_warnings=[],
        
        current_question=None,
        conversation_history=[],
        
        google_ai_studio_data=None,
        inferred_api_spec=None,
        
        final_trd=None,
        trd_validation_result=None,
        api_specification=None,
        database_schema=None,
        architecture_diagram=None,
        tech_stack_document=None,
        
        current_stage="initializing",
        iteration_count=0,
        completion_percentage=0.0,
        
        started_at=datetime.now(),
        completed_at=None,
        
        errors=[]
    )
```

---

## 4. ì¡°ê±´ë¶€ ë¶„ê¸° ë¡œì§

### 4.1 should_ask_user_clarification

```python
def should_ask_user_clarification(state: TechSpecState) -> str:
    """
    ì™„ì „ì„± ì ìˆ˜ì— ë”°ë¼ ë¶„ê¸°
    """
    if state["completeness_score"] < 80:
        return "need_clarification"
    else:
        return "proceed_to_gaps"
```

### 4.2 has_tech_gaps

```python
def has_tech_gaps(state: TechSpecState) -> str:
    """
    ê¸°ìˆ  gap ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    """
    if len(state["technical_gaps"]) > 0:
        return "research_needed"
    else:
        return "skip_to_code_parsing"
```

### 4.3 has_validation_conflict

```python
def has_validation_conflict(state: TechSpecState) -> str:
    """
    ì„ íƒ ê²€ì¦ ì‹œ ì¶©ëŒ ìˆëŠ”ì§€ í™•ì¸
    """
    if state.get("validation_warnings"):
        return "show_warning"
    else:
        return "no_conflict"
```

### 4.4 user_reselection_decision

```python
def user_reselection_decision(state: TechSpecState) -> str:
    """
    ì‚¬ìš©ìê°€ ì¬ì„ íƒ or ê³„ì† ì¤‘ ì„ íƒ
    WebSocket ë©”ì‹œì§€ë¡œ ê²°ì •
    """
    # ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦¼
    # LangGraphì˜ interrupt ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©
    pass
```

### 4.5 check_more_decisions

```python
def check_more_decisions(state: TechSpecState) -> str:
    """
    ì•„ì§ ê²°ì •ë˜ì§€ ì•Šì€ ê¸°ìˆ  gap ìˆëŠ”ì§€ í™•ì¸
    """
    all_gaps = state["technical_gaps"]
    selected = state["selected_technologies"]
    
    unresolved_gaps = [
        gap for gap in all_gaps 
        if gap["id"] not in selected
    ]
    
    if unresolved_gaps:
        return "more_research"
    else:
        return "all_decided"
```

### 4.6 is_trd_valid

```python
def is_trd_valid(state: TechSpecState) -> str:
    """
    TRD ê²€ì¦ ê²°ê³¼ í™•ì¸
    """
    validation = state.get("trd_validation_result")
    
    if validation and validation["is_valid"]:
        return "valid"
    else:
        # ì¬ìƒì„± íšŸìˆ˜ ì œí•œ
        if state["iteration_count"] >= 3:
            # 3íšŒ ì´ìƒ ì‹¤íŒ¨ ì‹œ ê°•ì œ í†µê³¼
            return "valid"
        else:
            state["iteration_count"] += 1
            return "invalid"
```

---

## 5. êµ¬í˜„ ì½”ë“œ

### 5.1 LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver

# ì²´í¬í¬ì¸í„° (ìƒíƒœ ì €ì¥ìš©)
memory = SqliteSaver.from_conn_string("tech_spec_agent.db")

def create_tech_spec_workflow():
    """Tech Spec Agent LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    workflow = StateGraph(TechSpecState)
    
    # ========== ë…¸ë“œ ì¶”ê°€ ==========
    workflow.add_node("load_inputs", load_inputs_node)
    workflow.add_node("analyze_completeness", analyze_completeness_node)
    workflow.add_node("ask_user_clarification", ask_user_clarification_node)
    workflow.add_node("identify_tech_gaps", identify_tech_gaps_node)
    workflow.add_node("research_technologies", research_technologies_node)
    workflow.add_node("present_options", present_options_node)
    workflow.add_node("wait_user_decision", wait_user_decision_node)
    workflow.add_node("validate_decision", validate_decision_node)
    workflow.add_node("warn_user", warn_user_node)
    workflow.add_node("parse_ai_studio_code", parse_ai_studio_code_node)
    workflow.add_node("infer_api_spec", infer_api_spec_node)
    workflow.add_node("generate_trd", generate_trd_node)
    workflow.add_node("validate_trd", validate_trd_node)
    workflow.add_node("generate_api_spec", generate_api_spec_node)
    workflow.add_node("generate_db_schema", generate_db_schema_node)
    workflow.add_node("generate_architecture", generate_architecture_node)
    workflow.add_node("generate_tech_stack_doc", generate_tech_stack_doc_node)
    workflow.add_node("save_to_db", save_to_db_node)
    workflow.add_node("notify_next_agent", notify_next_agent_node)
    
    # ========== ì—£ì§€ ì •ì˜ ==========
    
    # ì‹œì‘ì 
    workflow.set_entry_point("load_inputs")
    
    # ìˆœì°¨ ì—£ì§€
    workflow.add_edge("load_inputs", "analyze_completeness")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ 1: ì™„ì „ì„± ì²´í¬
    workflow.add_conditional_edges(
        "analyze_completeness",
        should_ask_user_clarification,
        {
            "need_clarification": "ask_user_clarification",
            "proceed_to_gaps": "identify_tech_gaps"
        }
    )
    
    # ëª…í™•í™” í›„ ì¬ë¶„ì„
    workflow.add_edge("ask_user_clarification", "analyze_completeness")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ 2: ê¸°ìˆ  gap ì¡´ì¬ ì—¬ë¶€
    workflow.add_conditional_edges(
        "identify_tech_gaps",
        has_tech_gaps,
        {
            "research_needed": "research_technologies",
            "skip_to_code_parsing": "parse_ai_studio_code"
        }
    )
    
    # ê¸°ìˆ  ì¡°ì‚¬ í”Œë¡œìš°
    workflow.add_edge("research_technologies", "present_options")
    workflow.add_edge("present_options", "wait_user_decision")
    workflow.add_edge("wait_user_decision", "validate_decision")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ 3: ê²€ì¦ ì¶©ëŒ
    workflow.add_conditional_edges(
        "validate_decision",
        has_validation_conflict,
        {
            "show_warning": "warn_user",
            "no_conflict": "check_more_decisions"
        }
    )
    
    # ê²½ê³  í›„ ì‚¬ìš©ì ê²°ì •
    workflow.add_conditional_edges(
        "warn_user",
        user_reselection_decision,
        {
            "reselect": "present_options",
            "continue": "check_more_decisions"
        }
    )
    
    # ì¡°ê±´ë¶€ ì—£ì§€ 4: ë” ë§ì€ ê²°ì • í•„ìš”?
    workflow.add_conditional_edges(
        "check_more_decisions",
        check_more_decisions,
        {
            "more_research": "research_technologies",
            "all_decided": "parse_ai_studio_code"
        }
    )
    
    # ì½”ë“œ ë¶„ì„ ë° ë¬¸ì„œ ìƒì„± í”Œë¡œìš°
    workflow.add_edge("parse_ai_studio_code", "infer_api_spec")
    workflow.add_edge("infer_api_spec", "generate_trd")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ 5: TRD ê²€ì¦
    workflow.add_conditional_edges(
        "validate_trd",
        is_trd_valid,
        {
            "valid": "generate_api_spec",
            "invalid": "generate_trd"
        }
    )
    
    # ë¬¸ì„œ ìƒì„± ì²´ì¸
    workflow.add_edge("generate_trd", "validate_trd")
    workflow.add_edge("generate_api_spec", "generate_db_schema")
    workflow.add_edge("generate_db_schema", "generate_architecture")
    workflow.add_edge("generate_architecture", "generate_tech_stack_doc")
    workflow.add_edge("generate_tech_stack_doc", "save_to_db")
    workflow.add_edge("save_to_db", "notify_next_agent")
    workflow.add_edge("notify_next_agent", END)
    
    # ì»´íŒŒì¼
    return workflow.compile(checkpointer=memory)

# ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
tech_spec_graph = create_tech_spec_workflow()
```

### 5.2 ì‹¤í–‰ ì˜ˆì‹œ

```python
# ìƒˆ ì„¸ì…˜ ì‹œì‘
initial_state = create_initial_state(
    project_id="proj_12345",
    user_id="user_67890"
)

# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ë¹„ë™ê¸°)
async def run_tech_spec_agent(initial_state):
    """Tech Spec Agent ì‹¤í–‰"""
    
    config = {
        "configurable": {
            "thread_id": initial_state["session_id"]
        }
    }
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    async for event in tech_spec_graph.astream(initial_state, config):
        # ê° ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼
        node_name = list(event.keys())[0]
        node_state = event[node_name]
        
        print(f"[{node_name}] Stage: {node_state['current_stage']}")
        print(f"[{node_name}] Progress: {node_state['completion_percentage']}%")
        
        # WebSocketìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ì†¡
        await send_to_frontend(initial_state["session_id"], {
            "type": "progress_update",
            "node": node_name,
            "stage": node_state["current_stage"],
            "progress": node_state["completion_percentage"],
            "message": node_state["conversation_history"][-1]["message"] if node_state["conversation_history"] else None
        })
        
        # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ë…¸ë“œì¸ ê²½ìš°
        if node_name in ["wait_user_decision", "ask_user_clarification"]:
            # Interrupt ë°œìƒ - ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
            print(f"Waiting for user input...")
            user_input = await wait_for_websocket_message(initial_state["session_id"])
            
            # ì‚¬ìš©ì ì…ë ¥ì„ ìƒíƒœì— ë°˜ì˜í•˜ê³  ì¬ê°œ
            node_state["user_input"] = user_input
            
    print("Tech Spec Agent completed!")
```

---

## 6. ë°ì´í„° íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    subgraph External["ì™¸ë¶€ ì‹œìŠ¤í…œ"]
        ANYON_DB[(ANYON DB)]
        S3[(S3 Storage)]
        WEB[Web Search]
        CLAUDE[Claude API]
    end
    
    subgraph TechSpecAgent["Tech Spec Agent"]
        LOAD[Load Inputs]
        ANALYZE[Analyze]
        RESEARCH[Research]
        SELECT[User Selection]
        PARSE[Parse Code]
        GENERATE[Generate Docs]
        SAVE[Save Results]
    end
    
    subgraph Frontend["í”„ë¡ íŠ¸ì—”ë“œ"]
        UI[Chat UI]
        KANBAN[Kanban Board]
    end
    
    ANYON_DB -->|PRD, ë””ìì¸| LOAD
    S3 -->|AI Studio Code| PARSE
    
    LOAD --> ANALYZE
    ANALYZE -->|Gap íƒì§€| RESEARCH
    
    RESEARCH <-->|ê²€ìƒ‰| WEB
    RESEARCH --> SELECT
    
    SELECT <-->|ì‚¬ìš©ì ì…ë ¥| UI
    
    SELECT --> PARSE
    PARSE --> GENERATE
    
    GENERATE <-->|ë¬¸ì„œ ìƒì„±| CLAUDE
    
    GENERATE --> SAVE
    SAVE --> ANYON_DB
    SAVE --> KANBAN
    
    style External fill:#e3f2fd
    style TechSpecAgent fill:#fff3e0
    style Frontend fill:#f3e5f5
```

---

## 7. ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

### 7.1 ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬

```python
class TechSpecError(Exception):
    """Base exception for Tech Spec Agent"""
    pass

class WebSearchFailedError(TechSpecError):
    """ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨"""
    pass

class CodeParsingError(TechSpecError):
    """ì½”ë“œ íŒŒì‹± ì‹¤íŒ¨"""
    pass

class DocumentGenerationError(TechSpecError):
    """ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨"""
    pass

async def handle_node_error(node_name: str, error: Exception, state: TechSpecState) -> TechSpecState:
    """
    ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ì²˜ë¦¬
    """
    error_log = {
        "node": node_name,
        "error_type": type(error).__name__,
        "message": str(error),
        "timestamp": datetime.now()
    }
    
    state["errors"].append(error_log)
    
    # ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
    if isinstance(error, WebSearchFailedError):
        # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ fallback
        state["conversation_history"].append({
            "role": "agent",
            "message": "ì›¹ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ìˆ  ì˜µì…˜ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now()
        })
        # ë¯¸ë¦¬ ì •ì˜ëœ ê¸°ìˆ  ìŠ¤íƒ í…œí”Œë¦¿ ì‚¬ìš©
        state["tech_research_results"].append(get_fallback_tech_options(state["technical_gaps"][-1]))
    
    elif isinstance(error, CodeParsingError):
        # ì½”ë“œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
        state["google_ai_studio_data"] = None
        state["conversation_history"].append({
            "role": "agent",
            "message": "ì½”ë“œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. PRDì™€ ë””ìì¸ ë¬¸ì„œë§Œìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now()
        })
    
    elif isinstance(error, DocumentGenerationError):
        # ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
        if state["iteration_count"] < 3:
            state["iteration_count"] += 1
            state["conversation_history"].append({
                "role": "agent",
                "message": f"ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤... ({state['iteration_count']}/3)",
                "timestamp": datetime.now()
            })
        else:
            raise  # 3íšŒ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì „íŒŒ
    
    # DBì— ì—ëŸ¬ ë¡œê·¸ ì €ì¥
    await db.execute("""
        INSERT INTO agent_error_logs (session_id, node, error_type, message)
        VALUES ($1, $2, $3, $4)
    """, state["session_id"], node_name, error_log["error_type"], error_log["message"])
    
    return state
```

### 7.2 ì²´í¬í¬ì¸íŠ¸ ë° ì¬ê°œ

```python
async def resume_from_checkpoint(session_id: str) -> TechSpecState:
    """
    ì¤‘ë‹¨ëœ ì„¸ì…˜ì„ ì¬ê°œ
    """
    # LangGraphì˜ ì²´í¬í¬ì¸í„°ì—ì„œ ë§ˆì§€ë§‰ ìƒíƒœ ë¡œë“œ
    config = {"configurable": {"thread_id": session_id}}
    last_state = await tech_spec_graph.aget_state(config)
    
    if last_state:
        # ë§ˆì§€ë§‰ ìƒíƒœì—ì„œ ì¬ê°œ
        return last_state.values
    else:
        raise ValueError(f"Session {session_id} not found")
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 8.1 ë³‘ë ¬ ì²˜ë¦¬

```python
async def research_multiple_gaps_parallel(gaps: List[Dict]) -> List[Dict]:
    """
    ì—¬ëŸ¬ ê¸°ìˆ  gapì„ ë³‘ë ¬ë¡œ ì¡°ì‚¬
    """
    tasks = [
        research_single_gap(gap) for gap in gaps
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ì—ëŸ¬ ì²˜ë¦¬
    valid_results = [
        result for result in results 
        if not isinstance(result, Exception)
    ]
    
    return valid_results
```

### 8.2 ìºì‹±

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
async def cached_web_search(query: str) -> List[Dict]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
    """
    # ë™ì¼í•œ ì¿¼ë¦¬ëŠ” ìºì‹œì—ì„œ ë°˜í™˜
    return await web_search(query)

async def get_cached_tech_research(gap_description: str) -> Optional[Dict]:
    """
    ì´ì „ ì¡°ì‚¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    """
    # Redis ìºì‹œì—ì„œ ì¡°íšŒ
    cache_key = f"tech_research:{hashlib.md5(gap_description.encode()).hexdigest()}"
    cached = await redis.get(cache_key)
    
    if cached:
        return json.loads(cached)
    else:
        return None
```

### 8.3 ìŠ¤íŠ¸ë¦¬ë°

```python
async def stream_trd_generation(state: TechSpecState) -> AsyncIterator[str]:
    """
    TRD ìƒì„±ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬
    """
    prompt = create_trd_prompt(state)
    
    async for chunk in claude_stream(prompt):
        # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì‹¤ì‹œê°„ ì „ì†¡
        await send_to_frontend(state["session_id"], {
            "type": "trd_chunk",
            "content": chunk
        })
        yield chunk
```

---

ì´ ë¬¸ì„œëŠ” Tech Spec Agentì˜ LangGraph êµ¬í˜„ì„ ì™„ë²½í•˜ê²Œ ì‹œê°í™”í•˜ê³  ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ì œ ê°œë°œíŒ€ì´ ë°”ë¡œ êµ¬í˜„ì— ì°©ìˆ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
